**DESAFIO BIG DATA/MODELAGEM**

üìå **ESCOPO DO DESAFIO**
Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta /raw onde vamos ter alguns arquivos .csv de um banco relacional de vendas.
s
 - VENDAS.CSV
 - CLIENTES.CSV
 - ENDERECO.CSV
 - REGIAO.CSV
 - DIVISAO.CSV

Seu trabalho como engenheiro de dados/arquiteto de BI √© prover dados em uma pasta desafio_curso/gold em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠do dentro da pasta 'app' (j√° tem o template).

üìë **ETAPAS**
**Etapa 1 -** Enviar os arquivos para o HDFS
    - nesta etapa lembre de criar um shell script para fazer o trabalho repetitivo (n√£o √© obrigat√≥rio)

**Etapa 2 -** Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.

    - DESAFIO_CURSO (nome do banco)
        - TBL_VENDAS
        - TBL_CLIENTES
        - TBL_ENDERECO
        - TBL_REGIAO
        - TBL_DIVISAO

**Etapa 3 -** Processar os dados no Spark Efetuando suas devidas transforma√ß√µes criando os arquivos com a modelagem de BI.
OBS. o desenvolvimento pode ser feito no jupyter porem no final o codigo deve estar no arquivo desafio_curso/scripts/process/process.py

**Etapa 4 -** Gravar as informa√ß√µes em tabelas dimensionais em formato cvs delimitado por ';'.

        - FT_VENDAS
        - DIM_CLIENTES
        - DIM_TEMPO
        - DIM_LOCALIDADE

**Etapa 5 -** Exportar os dados para a pasta desafio_curso/gold

**Etapa 6 -** Criar e editar o PowerBI com os dados que voc√™ trabalhou.

No PowerBI criar gr√°ficos de vendas.
**Etapa 7 -** Criar uma documenta√ß√£o com os testes e etapas do projeto.

**REGRAS**
Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.
Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.
Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.
Na tabela FATO, pelo menos a m√©trica <b>valor de venda</b> √© um requisito obrigat√≥rio.
Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.
para a dimens√£o tempo considerar o campo da TBL_VENDAS <b>Invoice Date</b>

## ‚úîÔ∏è ESTRUTURA E RESOLUTIVA

1. Para esse projeto foi criado no Github um fork no link a baixo, pegando todas as branchs em:  https://github.com/caiuafranca/bigdata_docker/tree/ambiente-curso

2. Criado um clone do ambiente-curso 

3. No gitpod, logado e apontando para seu projeto no github.

4. No gitpod, liberar todas as permiss√µes em:

5. Conta >  Gitpod: Open Acesson Control > Github > (...) > Marcar todos

6. Ao executar o docker-compose up ‚Äìd dentro do diret√≥rio bibdata_docker ser√° foi criado o diret√≥rio "input"

7. Realizar uploud da estrutura **desafio_curso**

8. **desafio_curso** tem a seguinte estrutura de diret√≥rios \n
   **app:** Diret√≥rio do arquivo de execu√ß√£o do **app/Projeto Vendas.pbix**.
   **config:** Diret√≥rio de do .sh de configura√ß√µes gerais do projeto
   **gold:** Diret√≥rio dos arquivos dimensionais
   **raw:** Diret√≥rio dos arquivos fonte de dados em .csv
   **run:** Diret√≥rio do PowerShell process que executar√° o processo process.py
   **scripts:** 
     **/hql:**  Diret√≥rio que conter√° os arquivos .hql. Cada arquivo tem tr√™s estruturas:
        - Cria√ß√£o das estruturas de banco Externa no Hive, fazendo uso dos dados do HDFS; 
        - Cria√ß√£o das tabelas Internas ou gerenciadas, tamb√©m dentro do Hive; 
        - Carga de dados nas tabelas gerenciadas a partir dos dados do HDFS, adicionando na carga um campo de versionamento da carga (dt_foto)
     **/hql:** Temos tamb√©m um .hql exclusivo para a cria√ß√£o dos bancos de dados que ser√£o utilizados   
     **/pre_process:** Diret√≥rio para armazenar os Powershell que:
        - **create_env_all.sh** - Cria estruturas no HDFS com base no nome dos arquivos contidos no diret√≥rio "raw"
                                - Aplica permiss√µes de manipula√ß√£o
                                - Copia os arquivos locais do diret√≥rio "raw" para o datalake\raw
        - **create_databases.sh** - Cria banco de dados
        - **create_env_charg_tables.sh** - Tem por finalidade, atrav√©s do comando "beeline" executar os .hql. √â por meio desse PowerShell que se passa todos os par√¢metros utilizados nos arquivos .hql devidamente sincronizados por arquivo/tabela. 
        - **insert_db_gold.sh** - Para esse powerShell ser√° usado as tabelas dimencionais que foram armazenadas no **/datalake/gold/** e ser√° criado as tabelas do banco dimencional e carregado os dados contidos no arquivo. Vale se atentar que ao realizar esse processo o hive move o arquivo que est√° na estrutura **/datalake/gold/**/ .csv para dentro do diret√≥rio raiz do banco **/user/hive/warehouse/desafio_curso_gold.db**
     **/process:** Nesse diret√≥rio encontra-se o arquivo process.py 
        - **process.py** - nesse arquivo encontra-se toda a estrutura spark e pyspark utilizada para o devido tratamento dos dados contidos no banco gerenciado. Ap√≥s todo o tratamento, √© realizado o processo de exportar os dados dimensionais gerados
     para o HDFS "datalake/gold" assim como √© armazenado de forma local no diret√≥rio "desafio_curso/gold".

**Ordem e local para executar os devidos comandos:**

0. dentro de \bigdata_docker executar ``docker-compose up -d`` para baixar e iniciar os container ou simplesmente ``docker-compose start`` para apenas iniciar os container
1. ``$ docker exec -it hive-server bash`` para acessar o container via bash 
2. root@hive_server:/input/desafio_curso/scripts/pre_process# ``./create_env_all.sh``
3. root@hive_server:/input/desafio_curso/scripts/pre_process# ``./create_databases.sh``   
4. root@hive_server:/input/desafio_curso/scripts/pre_process# ``./create_env_charg_tables.sh``
5. Em um novo terminal, ou fora do hive-server acessar o **jupyter-spark**  ``$ docker exec -it jupyter-spark bash``
6. root@jupyter-spark:/input/desafio_curso/run#  ``./process.sh``
7. root@hive_server:/input/desafio_curso/scripts/pre_process# ``./insert_db_gold.sh``
7. Abrir o app/Projeto Vendas.pbix

