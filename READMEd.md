# DESAFIO BIG DATA - TURMA INDRA UNIESP


## üìå ESCOPO DO DESAFIO
* Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta dados/dados_entrada onde vamos ter alguns arquivos .csv de um banco de vendas.

       - VENDAS.CSV
       - CLIENTES.CSV
       - ENDERECO.CSV
       - REGIAO.CSV
       - DIVISAO.CSV

* Seu trabalho como engenheiro de dados √© prover dados em uma pasta dados/dados_saida em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠di dentro da pasta 'app'.

### üìë ETAPAS

* **Etapa 1** - Enviar os arquivos para o HDFS.
* **Etapa 2** - Criar tabelas no Hive.

              - TBL_VENDAS
              - TBL_VENDAS_STG
              - TBL_CLIENTES
              - TBL_CLIENTES_STG
              - TBL_ENDERECO
              - TBL_ENDERECO_STG
              - TBL_REGIAO
              - TBL_REGIAO_STG
              - TBL_DIVISAO
              - TBL_DIVISAO_STG
* **Etapa 3** - Processar os dados no Spark Efetuando suas devidas transforma√ß√µes.
* **Etapa 4** - Gravar as informa√ß√µes em tabelas.

              - FT_VENDAS
              - DIM_CLIENTES
              - DIM_TEMPO
              - DIM_LOCALIDADE
* **Etapa 5** - Exportar os dados para a pasta dados/dados_saida
* **Etapa 6** - Criar e editar o PowerBI com os dados que voc√™ trabalhou.
  * No PowerBI criar gr√°ficos de vendas.
* **Etapa 7** - Criar uma documenta√ß√£o com os testes e etapas do projeto.

### REGRAS

* Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.
* Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.
* Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.
* Na tabela FATO, pelo menos a m√©trica valor de vnda √© um requisito obrigat√≥rio.
* Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.
   
### INSTRU√á√ïES

* Executar a malha do Hive dentro do Container hive-server.
* Executar o processamento do Spark dentro do container spark.


## ‚úîÔ∏è ESTRUTURA E RESOLUTIVA

Dentro do Container, aliadas as pastas j√° existentes, foram agregadas mais algumas pastas com scripts para execu√ß√£o do jobs da melhor forma.

Abaixo est√£o listadas as pastas e arquivos, representando a execu√ß√£o da Etapa 1, 2, 3, 4 e 5, com a descri√ß√£o do que representa cada conte√∫do:

* **app**: Pasta que constar√° o arquivo de execu√ß√£o do Power BI.
* **dados**
  * **dados_entrada**: Arquivos .csv que ser√£o inputadas dentro da Tabela Externa Hive no HDFS.
  * **dados_saida**: Arquivos .csv com as tabelas dimens√£o e FATO, ap√≥s tratamento via Spark, que ser√£o utilizadas para a reprodu√ß√£o no Power BI.
* **hqls**: Pasta que conter√° os arquivos .hql (scripts Hive) para execu√ß√£o. Cada tabela ter√° uma pasta diferente para fins de organiza√ß√£o. Cada pasta das tabelas ter√£o tr√™s arquivos, sendo: **create-external-table-stg** (respons√°vel por criar a planilha externa do HDFS com seus respectivos metadados); **create-managed-table-wrk** (respons√°vel por criar as tabelas internas tamb√©m dentro do Hive, mas que poder√° ser manipulada para seus devidos tratamentos); **insert-table-wrk** (script que levar√° os dados da tabela externa para a tabela interna, com o acr√©scimo da dt_foto, ou seja, a data da foto/informa√ß√µes).
  * **hql-clientes**
  * **hql-divisao**
  * **hql-endereco**
  * **hql-regiao**
  * **hql-vendas**
* **malha**: Pasta onde constar√° os scripts para ingest√£o das tabelas e execu√ß√£o do c√≥digo em Spark, nela tamb√©m estar√° o arquivo .py que realizar√° o tratamento em Spark da tabela interna.
  * *jobs_hive*: Script para executar outros scripts relativos √† cria√ß√£o da tabela externa e interna, com os seus devidos testes para confirma√ß√£o se os dados realmente foram inseridos nos conformes.
  * *jobs_spark*: Script para executar o c√≥digo criado em pyspark para tratamento dos dados da tabela interna e cria√ß√£o das dimens√µes e tabela FATO.
  * *job_processor*: Script python. Ele ser√° respons√°vel pelo tratamento dos dados da tabela interna.
* **scripts**: Pasta que conter√° todos os scripts em bash, para **cria√ß√£o de tabelas**, **inser√ß√£o dos dados na tabela externa e interna**.
  * **create_tables**: Pasta que conter√° os scripts para cria√ß√£o das 10 tabelas dentro do Hive (internas e externas), repassando as vari√°veis via Shell Script.
  * *insert_data_worked_table*: Script que passa vari√°veis e executa o .hql de inser√ß√£o de dados na tabela interna derivado da tabela externa.
  * *update_data_external_table*: Script que passa var√≠√°veis e informa o path do arquivo .csv para constru√ß√£o da tabela Externa. 
  * *job_processor*: Script python. Ele ser√° respons√°vel pelo tratamento dos dados da tabela interna.
* **tests**: Pasta que constar√° a valida√ß√£o do arquivo com um notebook Jupyter (Python).
* **utils**: Pasta com os a arquivos auxiliares do projeto, como, o desenho das dimens√µes em .pdf e edit√°veis, al√©m do notebook que foi trabalhado para chegar no c√≥digo final em script Python.
* *rollback*: Script para dele√ß√£o dos arquivos .csv do HDFS e das tabelas e BD do Hive, em caso de erro na execu√ß√£o das tabelas.
* *rollout*: Script para cria√ß√£o das pastas no HDFS e para chamar os scrips que criam as tabelas.

### üë®‚Äçüíª Rodando o c√≥digo
* *Instale os containers*: √â importante que voc√™ tenha instalado os containers do hive-server e spark para a execu√ß√£o do projeto. Abaixo segue o passo √† passo:
1. Baixar Docker Compose: ``$ sudo curl -L https://github.com/docker/compose/releases/download/1.28.2/docker-
compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose``

2. Alterar permiss√£o: ``$ sudo chmod +x /usr/local/bin/docker-compose``

3. Testar instala√ß√£o: ``$ docker-compose ‚Äìversion``

4. Baixar conteudo do Cluster: ``$ git clone https://github.com/tiandrefreitas/docker-bigdata.git``

5. Baixar as imagens: ``$ docker-compose pull``

6. Listar as imagens: ``$ docker image ls``

7. Executar os containers: ``$ docker-compose up ‚Äìd``

8. Ative os containers do Hive-Server e Spark (em terminais diferentes)
``
       $ docker exec -it hive-server bash
`` e 
``
$ docker exec -it spark bash
``
* *Baixer esse reposit√≥rio*: Ap√≥s setar o ambiente, dar um git clone nesse reposit√≥rio na pasta input, importante verificar se a pasta input local est√° sendo acess√≠vel por todos os containers envolvidos no processo.
* *Rollback*: Execute o script rollback para que as pastas sejam 'limpas' e tabelas desfeitas.
* *Rollout*: Execute o script rollout para cria√ß√£o de pastas no hdfs e tabelas.
* *jobs_hive*: Arquivo que se encontra na pasta 'malha' e que ir√° inserir os dados na tabela. 
* *jobs_spark*: Executar esse job no container spark para tratamento dos dados e migra√ß√£o das dimens√µes para a pasta do Unix.

### ‚öôÔ∏è Executando os testes
* O teste consiste em verificar se o quantitativo de vendas do arquivo inicial (retirando as linhas totalmente nulas), condiziam com a tabela FATO do fim, ap√≥s todo o tratamento. Foi utilizado a linguagem Python com condi√ß√µes que informariam se a valida√ß√£o foi ok ou n√£o.

### üìä Dashboard
[![hrmMkG.md.png](https://iili.io/hrmMkG.md.png)](https://freeimage.host/i/hrmMkG)

### üîß Ferramentas utilizadas
- ``Shell Script``
- ``Hive / HQL``
- ``Spark``
- ``Power BI``

### ‚úíÔ∏è Autor do desafio

* **Caiu√° Fran√ßa** - *desafio_bigdata_final* - [caiuafranca](https://github.com/caiuafranca/desafio_bigdata_final)
